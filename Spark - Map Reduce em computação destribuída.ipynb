!pip install pyspark
from pyspark import SparkContext
spark_contexto = SparkContext()
import numpy as np
vetor = np.array([10, 20, 30, 40, 50])
paralelo = spark_contexto.parallelize(vetor)
print(paralelo)
mapa = paralelo.map(lambda x : x**4-10*x**2+3)
mapa.collect()
paralelo = spark_contexto.parallelize(["distribuida", "distribuida", "spark", "rdd", "spark", "spark"])
funcao_lambda = lambda x:(x,1)
lista_rdd.count()
from operator import add
mapa = paralelo.map(funcao_lambda).reduceByKey(add).collect()
for (w, c) in mapa:
print("{}: {}".format(w, c))
spark_contexto.stop()
